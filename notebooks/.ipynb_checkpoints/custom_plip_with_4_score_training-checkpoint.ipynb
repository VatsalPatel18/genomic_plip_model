{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471c3f3b-0302-47f2-b685-7748ef442541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5baede-610a-4bf4-b54a-d8fec85afa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792a5fa-9229-4b68-bfb8-3bada939abfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5977d799-9da5-4434-8ac7-b6293db401f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tiles_per_patient = 595\n",
    "files = os.listdir('plip_preprocess/')\n",
    "train , test_val = train_test_split(files,test_size=0.4)\n",
    "test,val = train_test_split(test_val,test_size=0.5)\n",
    "for file in train[:]:\n",
    "    fol_p = os.path.join('plip_preprocess',file)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    selected_tiles = random.sample(tiles, min(num_tiles_per_patient, len(tiles)))\n",
    "    for tile in selected_tiles:\n",
    "        tile_p = os.path.join(fol_p,tile)\n",
    "        new_p = os.path.join('Datasets/train_03/train',tile)\n",
    "        shutil.copy(tile_p,new_p)\n",
    "\n",
    "for file in test[:]:\n",
    "    fol_p = os.path.join('plip_preprocess',file)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    selected_tiles = random.sample(tiles, min(num_tiles_per_patient, len(tiles)))\n",
    "    for tile in selected_tiles:\n",
    "        tile_p = os.path.join(fol_p,tile)\n",
    "        new_p = os.path.join('Datasets/train_03/test',tile)\n",
    "        shutil.copy(tile_p,new_p)\n",
    "\n",
    "for file in val[:]:\n",
    "    fol_p = os.path.join('plip_preprocess',file)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    selected_tiles = random.sample(tiles, min(num_tiles_per_patient, len(tiles)))\n",
    "    for tile in selected_tiles:\n",
    "        tile_p = os.path.join(fol_p,tile)\n",
    "        new_p = os.path.join('Datasets/train_03/validate',tile)\n",
    "        shutil.copy(tile_p,new_p)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176e91a8-5fcd-4243-9746-7cd67cb67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542c9278-dd07-4234-8de0-9e82fe9284c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tile_data', 'file_data'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/home/gp7/ml_pni/aug31/plip_preprocess/TCGA-CV-5966/TCGA-CV-5966_5_12.jpeg.pt').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19acf46-7935-4178-8df8-b03ce173a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/home/gp7/ml_pni/aug31/plip_preprocess/TCGA-CV-5966/TCGA-CV-5966_5_12.jpeg.pt')['tile_data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08af5bb9-dff2-421f-b258-4344b163e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('Datasets/train_03/train/TCGA-DQ-5625_2_10.jpeg.pt')['tile_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2785fa1a-05a3-4a17-8b3d-7ef15ddc2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "class FlatTileDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        data = torch.load(file_path)\n",
    "        tile_data = torch.from_numpy(data['tile_data'][0])\n",
    "        file_data = data['file_data']\n",
    "        return tile_data, file_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1494a5ea-eb01-4def-bdb0-19ee84fa2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FlatTileDataset(data_dir='Datasets/train_03/train')\n",
    "train_data_loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=32)\n",
    "\n",
    "validation_dataset = FlatTileDataset(data_dir='Datasets/train_03/validate')\n",
    "validation_data_loader = DataLoader(validation_dataset, batch_size=128, shuffle=False, num_workers=32)\n",
    "\n",
    "\n",
    "test_dataset = FlatTileDataset(data_dir='Datasets/train_03/test')\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71fa24aa-dab3-4902-b6cf-3b944d53b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel\n",
    "\n",
    "class CustomPLIPModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomPLIPModel, self).__init__()\n",
    "        self.vision_model = original_model.vision_model\n",
    "        self.vision_projection = torch.nn.Linear(768, 512)\n",
    "        self.fc_layer = torch.nn.Linear(4, 512)  # Fully connected layer for the 4D vector\n",
    "\n",
    "    def forward(self, pixel_values, score_vector):\n",
    "        vision_output = self.vision_model(pixel_values)\n",
    "        pooled_output = vision_output.pooler_output\n",
    "        vision_features = self.vision_projection(pooled_output)\n",
    "        score_features = self.fc_layer(score_vector)\n",
    "        \n",
    "        return vision_features, score_features\n",
    "    \n",
    "model = CLIPVisionModel.from_pretrained(\"../plip/\")\n",
    "custom_model = CustomPLIPModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd34f286-481d-4973-97a7-2b67b8faa335",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch part loss -55.4341\n",
      "Batch part loss -59.5735\n",
      "Batch part loss -62.1469\n",
      "Batch part loss -63.2711\n",
      "Batch part loss -63.3499\n",
      "Batch part loss -61.4691\n",
      "Batch part loss -65.3415\n",
      "Batch part loss -64.9783\n",
      "Batch part loss -68.6788\n",
      "Batch part loss -64.7548\n",
      "Batch part loss -66.9452\n",
      "Batch part loss -66.8607\n",
      "Batch part loss -69.3504\n",
      "Batch part loss -69.6781\n",
      "Batch part loss -71.0662\n",
      "Batch part loss -71.8540\n",
      "Batch part loss -67.4748\n",
      "Batch part loss -66.5620\n",
      "Batch part loss -73.9682\n",
      "Batch part loss -66.4608\n",
      "Batch part loss -70.6438\n",
      "Batch part loss -71.5281\n",
      "Batch part loss -73.7284\n",
      "Batch part loss -72.5080\n",
      "Batch part loss -71.1007\n",
      "Batch part loss -71.0023\n",
      "Batch part loss -68.8267\n",
      "Batch part loss -69.2588\n",
      "Batch part loss -68.8446\n",
      "Batch part loss -69.2159\n",
      "Batch part loss -67.4212\n",
      "Batch part loss -70.2703\n",
      "Batch part loss -68.1071\n",
      "Batch part loss -69.7616\n",
      "Batch part loss -67.9216\n",
      "Batch part loss -73.1598\n",
      "Batch part loss -71.2504\n",
      "Batch part loss -68.9539\n",
      "Batch part loss -69.1610\n",
      "Batch part loss -74.5227\n",
      "Batch part loss -76.0835\n",
      "Batch part loss -72.6858\n",
      "Batch part loss -74.3744\n",
      "Batch part loss -71.5973\n",
      "Batch part loss -68.7060\n",
      "Batch part loss -71.7376\n",
      "Batch part loss -68.4006\n",
      "Batch part loss -67.6465\n",
      "Batch part loss -65.9303\n",
      "Batch part loss -70.1220\n",
      "Batch part loss -72.2881\n",
      "Batch part loss -71.6799\n",
      "Batch part loss -70.9103\n",
      "Batch part loss -75.0347\n",
      "Batch part loss -72.4349\n",
      "Batch part loss -71.2624\n",
      "Batch part loss -73.7396\n",
      "Batch part loss -72.0502\n",
      "Batch part loss -69.6742\n",
      "Batch part loss -70.9944\n",
      "Batch part loss -73.8606\n",
      "Batch part loss -66.9856\n",
      "Batch part loss -72.8640\n",
      "Batch part loss -68.9324\n",
      "Batch part loss -72.3743\n",
      "Batch part loss -69.4890\n",
      "Batch part loss -73.5392\n",
      "Batch part loss -73.6227\n",
      "Batch part loss -73.8016\n",
      "Batch part loss -71.9642\n",
      "Batch part loss -70.0470\n",
      "Batch part loss -69.0653\n",
      "Batch part loss -67.1473\n",
      "Batch part loss -69.6415\n",
      "Batch part loss -72.4538\n",
      "Batch part loss -70.7447\n",
      "Batch part loss -72.1879\n",
      "Batch part loss -72.8840\n",
      "Batch part loss -71.8792\n",
      "Batch part loss -71.4270\n",
      "Batch part loss -70.3406\n",
      "Batch part loss -71.1080\n",
      "Batch part loss -76.5875\n",
      "Batch part loss -70.5601\n",
      "Batch part loss -70.7324\n",
      "Batch part loss -72.4096\n",
      "Batch part loss -73.7127\n",
      "Batch part loss -68.7584\n",
      "Batch part loss -67.3050\n",
      "Batch part loss -65.2810\n",
      "Batch part loss -70.1325\n",
      "Batch part loss -70.7387\n",
      "Batch part loss -69.4393\n",
      "Batch part loss -68.3571\n",
      "Batch part loss -70.3740\n",
      "Batch part loss -71.8683\n",
      "Batch part loss -70.0382\n",
      "Batch part loss -71.9747\n",
      "Batch part loss -74.6520\n",
      "Batch part loss -76.8453\n",
      "Batch part loss -74.4331\n",
      "Batch part loss -72.2088\n",
      "Batch part loss -75.1059\n",
      "Batch part loss -73.9926\n",
      "Batch part loss -73.3465\n",
      "Batch part loss -74.4346\n",
      "Batch part loss -71.4738\n",
      "Batch part loss -71.1114\n",
      "Batch part loss -72.7022\n",
      "Batch part loss -70.3643\n",
      "Batch part loss -74.1868\n",
      "Batch part loss -74.1549\n",
      "Batch part loss -69.2738\n",
      "Batch part loss -68.9777\n",
      "Batch part loss -75.8632\n",
      "Batch part loss -72.2308\n",
      "Batch part loss -70.4851\n",
      "Batch part loss -71.7389\n",
      "Batch part loss -74.6949\n",
      "Batch part loss -69.3898\n",
      "Batch part loss -73.3693\n",
      "Batch part loss -70.9850\n",
      "Batch part loss -72.9376\n",
      "Batch part loss -74.2818\n",
      "Batch part loss -72.3101\n",
      "Batch part loss -76.2324\n",
      "Batch part loss -73.4731\n",
      "Batch part loss -73.4158\n",
      "Batch part loss -69.9158\n",
      "Batch part loss -76.1740\n",
      "Batch part loss -77.4628\n",
      "Batch part loss -72.5659\n",
      "Batch part loss -76.0838\n",
      "Batch part loss -75.7269\n",
      "Batch part loss -72.3054\n",
      "Batch part loss -76.1887\n",
      "Batch part loss -73.3725\n",
      "Batch part loss -74.3032\n",
      "Batch part loss -71.2860\n",
      "Batch part loss -77.2158\n",
      "Batch part loss -70.0449\n",
      "Batch part loss -75.3496\n",
      "Batch part loss -74.3598\n",
      "Batch part loss -73.9363\n",
      "Batch part loss -75.6614\n",
      "Batch part loss -69.7068\n",
      "Batch part loss -71.6437\n",
      "Batch part loss -75.5877\n",
      "Batch part loss -73.4959\n",
      "Batch part loss -70.6447\n",
      "Batch part loss -76.6055\n",
      "Batch part loss -73.6871\n",
      "Batch part loss -74.7396\n",
      "Batch part loss -73.2814\n",
      "Batch part loss -71.5224\n",
      "Batch part loss -75.6547\n",
      "Batch part loss -75.0491\n",
      "Batch part loss -75.2051\n",
      "Batch part loss -73.3650\n",
      "Batch part loss -70.7282\n",
      "Batch part loss -75.8670\n",
      "Batch part loss -73.4204\n",
      "Batch part loss -73.5104\n",
      "Batch part loss -74.3601\n",
      "Batch part loss -71.9626\n",
      "Batch part loss -70.6834\n",
      "Batch part loss -76.4592\n",
      "Batch part loss -75.3755\n",
      "Batch part loss -80.4335\n",
      "Batch part loss -73.5084\n",
      "Batch part loss -75.9512\n",
      "Batch part loss -73.4319\n",
      "Batch part loss -76.2089\n",
      "Batch part loss -76.9678\n",
      "Batch part loss -77.0600\n",
      "Batch part loss -76.4674\n",
      "Batch part loss -76.0792\n",
      "Batch part loss -73.7338\n",
      "Batch part loss -77.9600\n",
      "Batch part loss -80.5319\n",
      "Batch part loss -75.3663\n",
      "Batch part loss -74.3596\n",
      "Batch part loss -74.1090\n",
      "Batch part loss -73.5524\n",
      "Batch part loss -77.8836\n",
      "Batch part loss -79.1235\n",
      "Batch part loss -74.5820\n",
      "Batch part loss -75.1815\n",
      "Batch part loss -75.5910\n",
      "Batch part loss -75.5506\n",
      "Batch part loss -75.8599\n",
      "Batch part loss -79.1416\n",
      "Batch part loss -79.8423\n",
      "Batch part loss -76.3790\n",
      "Batch part loss -76.0319\n",
      "Batch part loss -83.1419\n",
      "Batch part loss -77.2023\n",
      "Batch part loss -80.1275\n",
      "Batch part loss -78.9948\n",
      "Batch part loss -76.3684\n",
      "Batch part loss -76.3262\n",
      "Batch part loss -75.5534\n",
      "Batch part loss -74.6669\n",
      "Batch part loss -73.6642\n",
      "Batch part loss -72.6172\n",
      "Batch part loss -73.2060\n",
      "Batch part loss -72.9726\n",
      "Batch part loss -75.5384\n",
      "Batch part loss -77.5936\n",
      "Batch part loss -71.3863\n",
      "Batch part loss -74.9635\n",
      "Batch part loss -74.4325\n",
      "Batch part loss -74.9120\n",
      "Batch part loss -75.2518\n",
      "Batch part loss -75.1235\n",
      "Batch part loss -75.7129\n",
      "Batch part loss -76.6218\n",
      "Batch part loss -76.7155\n",
      "Batch part loss -74.7141\n",
      "Batch part loss -72.6879\n",
      "Batch part loss -76.8465\n",
      "Batch part loss -76.1606\n",
      "Batch part loss -76.7196\n",
      "Batch part loss -78.0874\n",
      "Batch part loss -74.6837\n",
      "Batch part loss -77.7312\n",
      "Batch part loss -73.0310\n",
      "Batch part loss -76.8814\n",
      "Batch part loss -75.9622\n",
      "Batch part loss -72.2105\n",
      "Batch part loss -74.2514\n",
      "Batch part loss -76.2475\n",
      "Batch part loss -76.7228\n",
      "Batch part loss -79.6799\n",
      "Batch part loss -77.6703\n",
      "Batch part loss -79.9050\n",
      "Batch part loss -80.1273\n",
      "Batch part loss -77.3809\n",
      "Batch part loss -78.0912\n",
      "Batch part loss -75.0597\n",
      "Batch part loss -79.4537\n",
      "Batch part loss -81.5844\n",
      "Batch part loss -77.7772\n",
      "Batch part loss -73.9373\n",
      "Batch part loss -81.1200\n",
      "Batch part loss -83.2387\n",
      "Batch part loss -77.4438\n",
      "Batch part loss -76.4084\n",
      "Batch part loss -75.9612\n",
      "Batch part loss -74.3504\n",
      "Batch part loss -78.2012\n",
      "Batch part loss -80.0144\n",
      "Batch part loss -78.2657\n",
      "Batch part loss -75.8390\n",
      "Batch part loss -78.7831\n",
      "Batch part loss -77.2181\n",
      "Batch part loss -78.4004\n",
      "Batch part loss -77.8435\n",
      "Batch part loss -81.7174\n",
      "Batch part loss -76.3667\n",
      "Batch part loss -75.1263\n",
      "Batch part loss -72.6129\n",
      "Batch part loss -82.3254\n",
      "Batch part loss -75.8781\n",
      "Batch part loss -75.4138\n",
      "Batch part loss -81.3457\n",
      "Batch part loss -74.4874\n",
      "Batch part loss -76.8304\n",
      "Batch part loss -78.6658\n",
      "Batch part loss -78.4907\n",
      "Batch part loss -78.3454\n",
      "Batch part loss -77.1649\n",
      "Batch part loss -76.5391\n",
      "Batch part loss -81.5938\n",
      "Batch part loss -76.0284\n",
      "Batch part loss -74.8439\n",
      "Batch part loss -77.7264\n",
      "Batch part loss -75.5254\n",
      "Batch part loss -77.2216\n",
      "Batch part loss -76.8941\n",
      "Batch part loss -75.7200\n",
      "Batch part loss -78.2980\n",
      "Batch part loss -81.3040\n",
      "Batch part loss -81.4919\n",
      "Batch part loss -79.7817\n",
      "Batch part loss -80.4546\n",
      "Batch part loss -80.1209\n",
      "Batch part loss -81.4596\n",
      "Batch part loss -77.1954\n",
      "Batch part loss -81.0945\n",
      "Batch part loss -81.5194\n",
      "Batch part loss -85.7648\n",
      "Batch part loss -81.7775\n",
      "Batch part loss -82.5621\n",
      "Batch part loss -75.6877\n",
      "Batch part loss -80.7627\n",
      "Batch part loss -83.7898\n",
      "Batch part loss -78.4975\n",
      "Batch part loss -81.4124\n",
      "Batch part loss -80.5490\n",
      "Batch part loss -85.4111\n",
      "Batch part loss -79.9158\n",
      "Batch part loss -78.9756\n",
      "Batch part loss -83.7061\n",
      "Batch part loss -83.7650\n",
      "Batch part loss -79.4281\n",
      "Batch part loss -76.0985\n",
      "Batch part loss -83.7022\n",
      "Batch part loss -84.2413\n",
      "Batch part loss -80.3512\n",
      "Batch part loss -80.8074\n",
      "Batch part loss -80.9447\n",
      "Batch part loss -80.7387\n",
      "Batch part loss -80.5823\n",
      "Batch part loss -84.1442\n",
      "Batch part loss -81.4026\n",
      "Batch part loss -79.0033\n",
      "Batch part loss -72.7111\n",
      "Batch part loss -85.3207\n",
      "Batch part loss -82.4924\n",
      "Batch part loss -79.4234\n",
      "Batch part loss -80.3083\n",
      "Batch part loss -80.4628\n",
      "Batch part loss -81.7656\n",
      "Batch part loss -79.8409\n",
      "Batch part loss -76.6057\n",
      "Batch part loss -76.3456\n",
      "Batch part loss -82.5048\n",
      "Batch part loss -80.6554\n",
      "Batch part loss -79.8427\n",
      "Batch part loss -78.6637\n",
      "Batch part loss -78.2852\n",
      "Batch part loss -75.5481\n",
      "Batch part loss -77.8617\n",
      "Batch part loss -85.1853\n",
      "Batch part loss -80.4990\n",
      "Batch part loss -79.8037\n",
      "Batch part loss -85.3107\n",
      "Batch part loss -85.7097\n",
      "Batch part loss -80.3956\n",
      "Batch part loss -82.8283\n",
      "Batch part loss -81.8962\n",
      "Batch part loss -81.8578\n",
      "Batch part loss -79.1680\n",
      "Batch part loss -87.4776\n",
      "Batch part loss -82.9726\n",
      "Batch part loss -82.6928\n",
      "Batch part loss -81.8425\n",
      "Batch part loss -86.3033\n",
      "Batch part loss -80.9676\n",
      "Batch part loss -81.1838\n",
      "Batch part loss -81.3022\n",
      "Batch part loss -80.4123\n",
      "Batch part loss -82.9578\n",
      "Batch part loss -81.6433\n",
      "Batch part loss -87.2279\n",
      "Batch part loss -82.8017\n",
      "Batch part loss -80.5081\n",
      "Batch part loss -86.0684\n",
      "Batch part loss -80.3959\n",
      "Batch part loss -80.5069\n",
      "Batch part loss -77.8941\n",
      "Batch part loss -82.9244\n",
      "Batch part loss -82.5764\n",
      "Batch part loss -84.0959\n",
      "Batch part loss -85.0123\n",
      "Batch part loss -82.7622\n",
      "Batch part loss -83.6704\n",
      "Batch part loss -81.7242\n",
      "Batch part loss -81.8259\n",
      "Batch part loss -78.8575\n",
      "Batch part loss -84.6214\n",
      "Batch part loss -80.6297\n",
      "Batch part loss -83.2041\n",
      "Batch part loss -74.8403\n",
      "Batch part loss -80.9650\n",
      "Batch part loss -86.1773\n",
      "Batch part loss -87.2262\n",
      "Batch part loss -81.7934\n",
      "Batch part loss -79.7957\n",
      "Batch part loss -87.0197\n",
      "Batch part loss -83.8192\n",
      "Batch part loss -82.8798\n",
      "Batch part loss -84.3673\n",
      "Batch part loss -83.0045\n",
      "Batch part loss -82.0997\n",
      "Batch part loss -82.3826\n",
      "Batch part loss -84.0874\n",
      "Batch part loss -79.1036\n",
      "Batch part loss -84.7027\n",
      "Batch part loss -84.1311\n",
      "Batch part loss -85.3961\n",
      "Batch part loss -82.4554\n",
      "Batch part loss -85.4302\n",
      "Batch part loss -83.3136\n",
      "Batch part loss -83.6820\n",
      "Batch part loss -83.6778\n",
      "Batch part loss -81.5272\n",
      "Batch part loss -82.9514\n",
      "Batch part loss -87.3432\n",
      "Batch part loss -86.4569\n",
      "Batch part loss -85.4168\n",
      "Batch part loss -86.8006\n",
      "Batch part loss -77.7596\n",
      "Batch part loss -84.0081\n",
      "Batch part loss -78.7326\n",
      "Batch part loss -86.2272\n",
      "Batch part loss -82.7487\n",
      "Batch part loss -83.3065\n",
      "Batch part loss -89.4767\n",
      "Batch part loss -79.0170\n",
      "Batch part loss -81.0725\n",
      "Batch part loss -81.7593\n",
      "Batch part loss -88.5610\n",
      "Batch part loss -92.2894\n",
      "Batch part loss -86.2549\n",
      "Batch part loss -84.6255\n",
      "Batch part loss -82.8647\n",
      "Batch part loss -77.6504\n",
      "Batch part loss -85.6785\n",
      "Batch part loss -83.9848\n",
      "Batch part loss -86.2830\n",
      "Batch part loss -84.4179\n",
      "Batch part loss -86.4784\n",
      "Batch part loss -83.2658\n",
      "Batch part loss -82.0247\n",
      "Batch part loss -86.1086\n",
      "Batch part loss -80.8579\n",
      "Batch part loss -85.8126\n",
      "Batch part loss -81.8458\n",
      "Batch part loss -87.0096\n",
      "Batch part loss -83.4443\n",
      "Batch part loss -83.4166\n",
      "Batch part loss -84.6463\n",
      "Batch part loss -79.6629\n",
      "Batch part loss -82.9885\n",
      "Batch part loss -83.6206\n",
      "Batch part loss -85.7571\n",
      "Batch part loss -84.9435\n",
      "Batch part loss -81.0558\n",
      "Batch part loss -79.1390\n",
      "Batch part loss -85.5918\n",
      "Batch part loss -87.5214\n",
      "Batch part loss -80.2728\n",
      "Batch part loss -85.3218\n",
      "Batch part loss -87.5720\n",
      "Batch part loss -84.7903\n",
      "Batch part loss -84.1694\n",
      "Batch part loss -87.4016\n",
      "Batch part loss -80.9717\n",
      "Batch part loss -85.4519\n",
      "Batch part loss -83.6313\n",
      "Batch part loss -79.0817\n",
      "Batch part loss -87.2744\n",
      "Batch part loss -82.6334\n",
      "Batch part loss -86.7717\n",
      "Batch part loss -89.9243\n",
      "Batch part loss -83.8202\n",
      "Batch part loss -83.4832\n",
      "Batch part loss -85.8125\n",
      "Batch part loss -87.2390\n",
      "Batch part loss -82.8349\n",
      "Batch part loss -84.5686\n",
      "Batch part loss -91.4144\n",
      "Batch part loss -87.5154\n",
      "Batch part loss -87.5440\n",
      "Batch part loss -86.9309\n",
      "Batch part loss -87.0847\n",
      "Batch part loss -88.8021\n",
      "Batch part loss -87.7073\n",
      "Batch part loss -86.8516\n",
      "Batch part loss -80.8265\n",
      "Batch part loss -88.1492\n",
      "Batch part loss -84.5357\n",
      "Batch part loss -85.8289\n",
      "Batch part loss -84.9666\n",
      "Batch part loss -90.9612\n",
      "Batch part loss -89.9553\n",
      "Batch part loss -84.7093\n",
      "Batch part loss -87.1103\n",
      "Batch part loss -88.4426\n",
      "Batch part loss -85.7467\n",
      "Batch part loss -90.2580\n",
      "Batch part loss -90.8172\n",
      "Batch part loss -84.4733\n",
      "Batch part loss -84.2144\n",
      "Batch part loss -86.6474\n",
      "Batch part loss -86.4322\n",
      "Batch part loss -87.4473\n",
      "Batch part loss -83.9307\n",
      "Batch part loss -89.2149\n",
      "Batch part loss -92.2514\n",
      "Batch part loss -82.1818\n",
      "Batch part loss -85.3599\n",
      "Batch part loss -87.0946\n",
      "Batch part loss -84.8900\n",
      "Batch part loss -83.8230\n",
      "Batch part loss -86.5001\n",
      "Batch part loss -93.0844\n",
      "Batch part loss -85.9248\n",
      "Batch part loss -92.4558\n",
      "Batch part loss -87.9537\n",
      "Batch part loss -89.6975\n",
      "Batch part loss -89.2310\n",
      "Batch part loss -84.9858\n",
      "Batch part loss -90.2449\n",
      "Batch part loss -88.4962\n",
      "Batch part loss -86.8289\n",
      "Batch part loss -93.7600\n",
      "Batch part loss -91.6818\n",
      "Batch part loss -90.7685\n",
      "Batch part loss -86.2346\n",
      "Batch part loss -89.1852\n",
      "Batch part loss -91.6946\n",
      "Batch part loss -90.2165\n",
      "Batch part loss -89.9308\n",
      "Batch part loss -92.6184\n",
      "Batch part loss -91.0510\n",
      "Batch part loss -91.1405\n",
      "Batch part loss -88.8156\n",
      "Batch part loss -83.6349\n",
      "Batch part loss -85.1673\n",
      "Batch part loss -90.1984\n",
      "Batch part loss -87.0342\n",
      "Batch part loss -91.3203\n",
      "Batch part loss -90.9137\n",
      "Batch part loss -92.4377\n",
      "Batch part loss -87.7731\n",
      "Batch part loss -90.8524\n",
      "Batch part loss -89.5863\n",
      "Batch part loss -91.0244\n",
      "Batch part loss -84.6147\n",
      "Batch part loss -85.9647\n",
      "Batch part loss -92.4466\n",
      "Batch part loss -88.4039\n",
      "Batch part loss -89.8640\n",
      "Batch part loss -90.6206\n",
      "Batch part loss -86.2037\n",
      "Batch part loss -97.1780\n",
      "Batch part loss -91.4132\n",
      "Batch part loss -95.5094\n",
      "Batch part loss -91.5744\n",
      "Batch part loss -87.9444\n",
      "Batch part loss -88.6869\n",
      "Batch part loss -90.6585\n",
      "Batch part loss -86.7474\n",
      "Batch part loss -89.9874\n",
      "Batch part loss -89.7821\n",
      "Batch part loss -90.8203\n",
      "Batch part loss -90.2129\n",
      "Batch part loss -88.3705\n",
      "Batch part loss -81.3165\n",
      "Batch part loss -92.5502\n",
      "Batch part loss -86.2517\n",
      "Batch part loss -92.3862\n",
      "Batch part loss -88.1752\n",
      "Batch part loss -91.5202\n",
      "Batch part loss -90.8447\n",
      "Batch part loss -91.7923\n",
      "Batch part loss -91.2477\n",
      "Batch part loss -89.1964\n",
      "Batch part loss -92.0938\n",
      "Batch part loss -87.2189\n",
      "Batch part loss -87.3399\n",
      "Batch part loss -87.1049\n",
      "Batch part loss -93.3342\n",
      "Batch part loss -89.5199\n",
      "Batch part loss -90.7903\n",
      "Batch part loss -86.3236\n",
      "Batch part loss -91.8121\n",
      "Batch part loss -92.1494\n",
      "Batch part loss -95.6715\n",
      "Batch part loss -87.7023\n",
      "Batch part loss -93.5709\n",
      "Batch part loss -86.2459\n",
      "Batch part loss -92.6552\n",
      "Batch part loss -90.5339\n",
      "Batch part loss -87.9812\n",
      "Batch part loss -88.9954\n",
      "Batch part loss -87.5134\n",
      "Batch part loss -90.4062\n",
      "Batch part loss -87.5338\n",
      "Batch part loss -91.5134\n",
      "Batch part loss -88.2453\n",
      "Batch part loss -91.2315\n",
      "Batch part loss -88.5880\n",
      "Batch part loss -89.2943\n",
      "Batch part loss -89.0157\n",
      "Batch part loss -89.8352\n",
      "Batch part loss -90.6491\n",
      "Batch part loss -93.6112\n",
      "Batch part loss -91.2050\n",
      "Batch part loss -97.0666\n",
      "Batch part loss -90.9746\n",
      "Batch part loss -90.5283\n",
      "Batch part loss -92.1018\n",
      "Batch part loss -92.3035\n",
      "Batch part loss -92.0094\n",
      "Batch part loss -89.2401\n",
      "Batch part loss -92.0511\n",
      "Batch part loss -96.0878\n",
      "Batch part loss -94.1760\n",
      "Batch part loss -89.8993\n",
      "Batch part loss -88.5950\n",
      "Batch part loss -97.1218\n",
      "Batch part loss -92.4426\n",
      "Batch part loss -94.5225\n",
      "Batch part loss -90.1351\n",
      "Batch part loss -95.5458\n",
      "Batch part loss -91.6802\n",
      "Batch part loss -89.8815\n",
      "Batch part loss -88.6694\n",
      "Batch part loss -91.0289\n",
      "Batch part loss -93.3789\n",
      "Batch part loss -91.0623\n",
      "Batch part loss -94.3532\n",
      "Batch part loss -95.6260\n",
      "Batch part loss -94.2706\n",
      "Batch part loss -90.8733\n",
      "Batch part loss -90.8730\n",
      "Batch part loss -94.3599\n",
      "Batch part loss -93.7214\n",
      "Batch part loss -88.5930\n",
      "Batch part loss -88.9140\n",
      "Batch part loss -93.1144\n",
      "Batch part loss -89.8350\n",
      "Batch part loss -91.0591\n",
      "Batch part loss -91.2954\n",
      "Batch part loss -93.4777\n",
      "Batch part loss -90.6536\n",
      "Batch part loss -88.8076\n",
      "Batch part loss -86.9141\n",
      "Batch part loss -93.1256\n",
      "Batch part loss -89.4812\n",
      "Batch part loss -96.7616\n",
      "Batch part loss -92.0970\n",
      "Batch part loss -91.3250\n",
      "Batch part loss -91.1363\n",
      "Batch part loss -86.9602\n",
      "Batch part loss -96.7312\n",
      "Batch part loss -97.4083\n",
      "Batch part loss -90.3518\n",
      "Batch part loss -90.5022\n",
      "Batch part loss -91.3894\n",
      "Batch part loss -94.4306\n",
      "Batch part loss -90.7152\n",
      "Batch part loss -94.7187\n",
      "Batch part loss -96.5821\n",
      "Batch part loss -89.0305\n",
      "Batch part loss -93.8540\n",
      "Batch part loss -98.7911\n",
      "Batch part loss -91.1367\n",
      "Batch part loss -97.6023\n",
      "Batch part loss -90.2461\n",
      "Batch part loss -94.2640\n",
      "Batch part loss -92.8980\n",
      "Batch part loss -96.9786\n",
      "Batch part loss -92.6427\n",
      "Batch part loss -93.7975\n",
      "Batch part loss -95.5851\n",
      "Batch part loss -92.6608\n",
      "Batch part loss -92.5590\n",
      "Batch part loss -89.9937\n",
      "Batch part loss -89.9829\n",
      "Batch part loss -94.2935\n",
      "Batch part loss -92.7262\n",
      "Batch part loss -94.6221\n",
      "Batch part loss -98.8005\n",
      "Batch part loss -94.4715\n",
      "Batch part loss -95.0537\n",
      "Batch part loss -92.3649\n",
      "Batch part loss -92.1957\n",
      "Batch part loss -92.8694\n",
      "Batch part loss -90.1440\n",
      "Batch part loss -91.0895\n",
      "Batch part loss -94.8070\n",
      "Batch part loss -92.8593\n",
      "Batch part loss -89.5018\n",
      "Batch part loss -94.8354\n",
      "Batch part loss -94.6599\n",
      "Batch part loss -96.9596\n",
      "Batch part loss -95.2955\n",
      "Batch part loss -93.6001\n",
      "Batch part loss -95.2203\n",
      "Batch part loss -98.7468\n",
      "Batch part loss -96.8957\n",
      "Batch part loss -94.3153\n",
      "Batch part loss -100.6366\n",
      "Batch part loss -94.5795\n",
      "Batch part loss -95.2049\n",
      "Batch part loss -95.6425\n",
      "Batch part loss -85.7623\n",
      "Batch part loss -92.9484\n",
      "Batch part loss -92.9766\n",
      "Batch part loss -91.7023\n",
      "Batch part loss -93.9150\n",
      "Batch part loss -89.7174\n",
      "Batch part loss -93.9268\n",
      "Batch part loss -93.6129\n",
      "Batch part loss -101.0808\n",
      "Batch part loss -92.7657\n",
      "Batch part loss -96.6213\n",
      "Batch part loss -94.7128\n",
      "Batch part loss -92.3579\n",
      "Batch part loss -96.0289\n",
      "Batch part loss -99.6069\n",
      "Batch part loss -96.7474\n",
      "Batch part loss -95.1475\n",
      "Batch part loss -89.0640\n",
      "Batch part loss -95.0755\n",
      "Batch part loss -95.8595\n",
      "Batch part loss -92.4148\n",
      "Batch part loss -94.7171\n",
      "Batch part loss -97.1814\n",
      "Batch part loss -96.4895\n",
      "Batch part loss -99.8571\n",
      "Batch part loss -96.4363\n",
      "Batch part loss -91.7906\n",
      "Batch part loss -95.9075\n",
      "Batch part loss -91.2424\n",
      "Batch part loss -98.7968\n",
      "Batch part loss -93.1926\n",
      "Batch part loss -92.4459\n",
      "Batch part loss -94.4054\n",
      "Batch part loss -88.6334\n",
      "Batch part loss -100.3468\n",
      "Batch part loss -93.9209\n",
      "Batch part loss -94.2741\n",
      "Batch part loss -98.9094\n",
      "Batch part loss -100.1326\n",
      "Batch part loss -95.1472\n",
      "Batch part loss -90.8516\n",
      "Batch part loss -96.6440\n",
      "Batch part loss -100.0345\n",
      "Batch part loss -95.3595\n",
      "Batch part loss -92.9036\n",
      "Batch part loss -95.0482\n",
      "Batch part loss -96.2442\n",
      "Batch part loss -94.9374\n",
      "Batch part loss -94.3551\n",
      "Batch part loss -95.9354\n",
      "Batch part loss -95.8195\n",
      "Batch part loss -100.0153\n",
      "Batch part loss -98.0534\n",
      "Batch part loss -96.4629\n",
      "Batch part loss -99.8435\n",
      "Batch part loss -96.9171\n",
      "Batch part loss -99.2727\n",
      "Batch part loss -101.5064\n",
      "Batch part loss -96.2119\n",
      "Batch part loss -97.9116\n",
      "Batch part loss -95.2595\n",
      "Batch part loss -95.3279\n",
      "Batch part loss -95.5055\n",
      "Batch part loss -98.7896\n",
      "Batch part loss -94.7734\n",
      "Batch part loss -98.4121\n",
      "Batch part loss -97.3286\n",
      "Batch part loss -97.5494\n",
      "Batch part loss -96.8415\n",
      "Batch part loss -92.6077\n",
      "Batch part loss -98.4360\n",
      "Batch part loss -91.4819\n",
      "Batch part loss -94.6579\n",
      "Batch part loss -98.7343\n",
      "Batch part loss -92.7147\n",
      "Batch part loss -97.9559\n",
      "Batch part loss -91.5998\n",
      "Batch part loss -94.1133\n",
      "Batch part loss -91.2180\n",
      "Batch part loss -94.9552\n",
      "Batch part loss -98.1235\n",
      "Batch part loss -93.5054\n",
      "Batch part loss -95.5100\n",
      "Batch part loss -93.8371\n",
      "Batch part loss -98.6740\n",
      "Batch part loss -97.6116\n",
      "Batch part loss -100.1141\n",
      "Batch part loss -94.7158\n",
      "Batch part loss -95.5349\n",
      "Batch part loss -95.1622\n",
      "Batch part loss -97.3136\n",
      "Batch part loss -100.2103\n",
      "Batch part loss -95.3073\n",
      "Batch part loss -92.2525\n",
      "Batch part loss -100.3046\n",
      "Batch part loss -93.8918\n",
      "Batch part loss -97.8502\n",
      "Batch part loss -98.7806\n",
      "Batch part loss -95.5231\n",
      "Batch part loss -98.1913\n",
      "Batch part loss -95.2932\n",
      "Batch part loss -97.7409\n",
      "Batch part loss -94.6515\n",
      "Batch part loss -102.7219\n",
      "Batch part loss -97.9706\n",
      "Batch part loss -95.2338\n",
      "Batch part loss -93.9559\n",
      "Batch part loss -95.7390\n",
      "Batch part loss -91.2434\n",
      "Batch part loss -96.3656\n",
      "Batch part loss -101.3832\n",
      "Batch part loss -95.8571\n",
      "Batch part loss -96.4748\n",
      "Batch part loss -92.2077\n",
      "Batch part loss -96.6306\n",
      "Batch part loss -99.3136\n",
      "Batch part loss -100.2379\n",
      "Batch part loss -96.0656\n",
      "Batch part loss -99.7633\n",
      "Batch part loss -97.6953\n",
      "Batch part loss -98.2946\n",
      "Batch part loss -96.4771\n",
      "Batch part loss -99.1868\n",
      "Batch part loss -99.6397\n",
      "Batch part loss -96.5940\n",
      "Batch part loss -100.6491\n",
      "Batch part loss -95.4552\n",
      "Batch part loss -93.1760\n",
      "Batch part loss -94.7840\n",
      "Batch part loss -95.1007\n",
      "Batch part loss -95.9917\n",
      "Batch part loss -98.9395\n",
      "Batch part loss -99.3995\n",
      "Batch part loss -95.8871\n",
      "Batch part loss -95.5998\n",
      "Batch part loss -94.6918\n",
      "Batch part loss -98.2554\n",
      "Batch part loss -98.3959\n",
      "Batch part loss -90.1774\n",
      "Batch part loss -92.3486\n",
      "Batch part loss -92.7122\n",
      "Batch part loss -98.0779\n",
      "Batch part loss -95.1488\n",
      "Batch part loss -95.0512\n",
      "Batch part loss -90.9097\n",
      "Batch part loss -93.6895\n",
      "Batch part loss -96.9132\n",
      "Batch part loss -95.2361\n",
      "Batch part loss -96.9743\n",
      "Batch part loss -98.7928\n",
      "Batch part loss -95.5115\n",
      "Batch part loss -100.4492\n",
      "Batch part loss -93.5391\n",
      "Batch part loss -96.3421\n",
      "Batch part loss -98.2708\n",
      "Batch part loss -93.7881\n",
      "Batch part loss -98.8669\n",
      "Batch part loss -96.0154\n",
      "Batch part loss -92.9892\n",
      "Batch part loss -97.3313\n",
      "Batch part loss -99.0395\n",
      "Batch part loss -99.6469\n",
      "Batch part loss -95.9414\n",
      "Batch part loss -95.4168\n",
      "Batch part loss -99.7184\n",
      "Batch part loss -100.8293\n",
      "Batch part loss -98.3993\n",
      "Batch part loss -95.7926\n",
      "Batch part loss -98.7753\n",
      "Batch part loss -92.8753\n",
      "Batch part loss -95.8831\n",
      "Batch part loss -95.6072\n",
      "Batch part loss -97.3848\n",
      "Batch part loss -97.8155\n",
      "Batch part loss -95.4058\n",
      "Batch part loss -99.0120\n",
      "Batch part loss -97.4076\n",
      "Batch part loss -101.9024\n",
      "Batch part loss -96.6549\n",
      "Batch part loss -101.3691\n",
      "Batch part loss -97.9955\n",
      "Batch part loss -102.2031\n",
      "Batch part loss -101.7479\n",
      "Batch part loss -96.8537\n",
      "Batch part loss -100.4971\n",
      "Batch part loss -100.5584\n",
      "Batch part loss -100.1095\n",
      "Batch part loss -102.3361\n",
      "Batch part loss -95.3941\n",
      "Batch part loss -94.0052\n",
      "Batch part loss -96.7944\n",
      "Batch part loss -103.0684\n",
      "Batch part loss -102.9814\n",
      "Batch part loss -97.8253\n",
      "Batch part loss -99.7066\n",
      "Batch part loss -100.4298\n",
      "Batch part loss -100.9464\n",
      "Batch part loss -103.8816\n",
      "Batch part loss -97.5404\n",
      "Batch part loss -98.0097\n",
      "Batch part loss -100.1900\n",
      "Batch part loss -100.3281\n",
      "Batch part loss -101.3794\n",
      "Batch part loss -102.9485\n",
      "Batch part loss -102.6219\n",
      "Batch part loss -99.2604\n",
      "Batch part loss -100.6843\n",
      "Batch part loss -95.7598\n",
      "Batch part loss -99.5854\n",
      "Batch part loss -97.7291\n",
      "Batch part loss -97.1949\n",
      "Batch part loss -95.5014\n",
      "Batch part loss -95.9896\n",
      "Batch part loss -96.4925\n",
      "Batch part loss -96.3079\n",
      "Batch part loss -100.8370\n",
      "Batch part loss -97.7764\n",
      "Batch part loss -98.2856\n",
      "Batch part loss -20.2032\n",
      "Epoch [1/1], Training Loss: -85.1273\n",
      "Validation Batch part loss -56.3884\n",
      "Validation Batch part loss -102.0471\n",
      "Validation Batch part loss -57.8282\n",
      "Validation Batch part loss -106.2255\n",
      "Validation Batch part loss -86.9873\n",
      "Validation Batch part loss -99.4780\n",
      "Validation Batch part loss -78.9556\n",
      "Validation Batch part loss -108.9473\n",
      "Validation Batch part loss -105.5480\n",
      "Validation Batch part loss -68.8977\n",
      "Validation Batch part loss -94.2508\n",
      "Validation Batch part loss -96.6541\n",
      "Validation Batch part loss -96.9153\n",
      "Validation Batch part loss -102.4418\n",
      "Validation Batch part loss -94.5976\n",
      "Validation Batch part loss -105.3241\n",
      "Validation Batch part loss -84.1197\n",
      "Validation Batch part loss -115.4344\n",
      "Validation Batch part loss -75.0933\n",
      "Validation Batch part loss -103.9393\n",
      "Validation Batch part loss -100.5949\n",
      "Validation Batch part loss -114.2502\n",
      "Validation Batch part loss -99.8472\n",
      "Validation Batch part loss -102.2925\n",
      "Validation Batch part loss -77.5032\n",
      "Validation Batch part loss -93.0064\n",
      "Validation Batch part loss -60.3222\n",
      "Validation Batch part loss -77.6956\n",
      "Validation Batch part loss -93.6630\n",
      "Validation Batch part loss -83.9265\n",
      "Validation Batch part loss -53.9977\n",
      "Validation Batch part loss -49.3083\n",
      "Validation Batch part loss -87.2601\n",
      "Validation Batch part loss -80.2503\n",
      "Validation Batch part loss -65.4120\n",
      "Validation Batch part loss -86.5085\n",
      "Validation Batch part loss -40.6641\n",
      "Validation Batch part loss -78.3950\n",
      "Validation Batch part loss -91.2160\n",
      "Validation Batch part loss -116.5507\n",
      "Validation Batch part loss -104.7197\n",
      "Validation Batch part loss -100.3431\n",
      "Validation Batch part loss -94.6025\n",
      "Validation Batch part loss -91.4345\n",
      "Validation Batch part loss -63.8315\n",
      "Validation Batch part loss -57.8674\n",
      "Validation Batch part loss -54.3642\n",
      "Validation Batch part loss -89.4163\n",
      "Validation Batch part loss -74.2816\n",
      "Validation Batch part loss -93.4525\n",
      "Validation Batch part loss -94.7759\n",
      "Validation Batch part loss -68.4831\n",
      "Validation Batch part loss -97.4930\n",
      "Validation Batch part loss -104.6555\n",
      "Validation Batch part loss -83.8405\n",
      "Validation Batch part loss -92.0547\n",
      "Validation Batch part loss -107.4320\n",
      "Validation Batch part loss -101.3167\n",
      "Validation Batch part loss -112.4016\n",
      "Validation Batch part loss -100.0126\n",
      "Validation Batch part loss -112.3400\n",
      "Validation Batch part loss -96.9650\n",
      "Validation Batch part loss -43.5006\n",
      "Validation Batch part loss -91.0849\n",
      "Validation Batch part loss -73.1173\n",
      "Validation Batch part loss -79.6242\n",
      "Validation Batch part loss -49.1348\n",
      "Validation Batch part loss -102.4158\n",
      "Validation Batch part loss -115.3498\n",
      "Validation Batch part loss -102.3315\n",
      "Validation Batch part loss -71.5874\n",
      "Validation Batch part loss -114.0281\n",
      "Validation Batch part loss -81.7327\n",
      "Validation Batch part loss -74.4404\n",
      "Validation Batch part loss -80.1187\n",
      "Validation Batch part loss -73.3417\n",
      "Validation Batch part loss -72.8402\n",
      "Validation Batch part loss -69.9399\n",
      "Validation Batch part loss -108.9193\n",
      "Validation Batch part loss -113.4653\n",
      "Validation Batch part loss -107.3628\n",
      "Validation Batch part loss -104.8194\n",
      "Validation Batch part loss -103.2639\n",
      "Validation Batch part loss -106.2976\n",
      "Validation Batch part loss -106.5716\n",
      "Validation Batch part loss -99.2724\n",
      "Validation Batch part loss -64.4821\n",
      "Validation Batch part loss -77.5550\n",
      "Validation Batch part loss -80.6094\n",
      "Validation Batch part loss -79.8049\n",
      "Validation Batch part loss -80.3391\n",
      "Validation Batch part loss -55.4525\n",
      "Validation Batch part loss -55.2204\n",
      "Validation Batch part loss -68.4940\n",
      "Validation Batch part loss -86.1745\n",
      "Validation Batch part loss -86.7063\n",
      "Validation Batch part loss -84.7786\n",
      "Validation Batch part loss -75.3437\n",
      "Validation Batch part loss -66.8778\n",
      "Validation Batch part loss -66.7931\n",
      "Validation Batch part loss -65.1922\n",
      "Validation Batch part loss -66.4615\n",
      "Validation Batch part loss -63.1521\n",
      "Validation Batch part loss -58.2089\n",
      "Validation Batch part loss -56.0270\n",
      "Validation Batch part loss -57.5790\n",
      "Validation Batch part loss -57.3288\n",
      "Validation Batch part loss -61.8139\n",
      "Validation Batch part loss -57.2177\n",
      "Validation Batch part loss -60.8972\n",
      "Validation Batch part loss -58.6967\n",
      "Validation Batch part loss -91.2051\n",
      "Validation Batch part loss -110.0312\n",
      "Validation Batch part loss -110.8262\n",
      "Validation Batch part loss -110.3823\n",
      "Validation Batch part loss -110.2412\n",
      "Validation Batch part loss -76.5506\n",
      "Validation Batch part loss -75.1285\n",
      "Validation Batch part loss -74.8855\n",
      "Validation Batch part loss -76.1199\n",
      "Validation Batch part loss -77.9633\n",
      "Validation Batch part loss -95.8483\n",
      "Validation Batch part loss -95.6347\n",
      "Validation Batch part loss -93.1634\n",
      "Validation Batch part loss -90.7816\n",
      "Validation Batch part loss -82.6541\n",
      "Validation Batch part loss -91.4418\n",
      "Validation Batch part loss -92.6370\n",
      "Validation Batch part loss -76.5430\n",
      "Validation Batch part loss -47.6459\n",
      "Validation Batch part loss -43.7570\n",
      "Validation Batch part loss -47.6161\n",
      "Validation Batch part loss -51.2298\n",
      "Validation Batch part loss -46.3687\n",
      "Validation Batch part loss -48.8115\n",
      "Validation Batch part loss -51.1676\n",
      "Validation Batch part loss -72.2752\n",
      "Validation Batch part loss -73.3411\n",
      "Validation Batch part loss -77.6625\n",
      "Validation Batch part loss -81.6145\n",
      "Validation Batch part loss -77.5824\n",
      "Validation Batch part loss -76.9716\n",
      "Validation Batch part loss -87.5066\n",
      "Validation Batch part loss -97.7577\n",
      "Validation Batch part loss -98.4956\n",
      "Validation Batch part loss -99.2016\n",
      "Validation Batch part loss -99.3913\n",
      "Validation Batch part loss -97.2270\n",
      "Validation Batch part loss -95.9886\n",
      "Validation Batch part loss -96.4968\n",
      "Validation Batch part loss -96.1306\n",
      "Validation Batch part loss -93.9996\n",
      "Validation Batch part loss -45.8584\n",
      "Validation Batch part loss -44.9524\n",
      "Validation Batch part loss -86.2907\n",
      "Validation Batch part loss -96.7779\n",
      "Validation Batch part loss -93.1289\n",
      "Validation Batch part loss -91.9107\n",
      "Validation Batch part loss -38.2933\n",
      "Validation Batch part loss -28.1616\n",
      "Validation Batch part loss -33.1994\n",
      "Validation Batch part loss -27.2286\n",
      "Validation Batch part loss -42.3006\n",
      "Validation Batch part loss -92.8781\n",
      "Validation Batch part loss -96.4653\n",
      "Validation Batch part loss -92.6328\n",
      "Validation Batch part loss -91.1727\n",
      "Validation Batch part loss -73.1711\n",
      "Validation Batch part loss -90.0543\n",
      "Validation Batch part loss -97.2015\n",
      "Validation Batch part loss -97.6422\n",
      "Validation Batch part loss -94.5690\n",
      "Validation Batch part loss -84.6064\n",
      "Validation Batch part loss -37.4982\n",
      "Validation Batch part loss -44.4359\n",
      "Validation Batch part loss -37.1345\n",
      "Validation Batch part loss -36.0669\n",
      "Validation Batch part loss -32.0517\n",
      "Validation Batch part loss -18.0494\n",
      "Validation Batch part loss -22.3521\n",
      "Validation Batch part loss -16.8288\n",
      "Validation Batch part loss -22.9044\n",
      "Validation Batch part loss -34.4494\n",
      "Validation Batch part loss -49.7731\n",
      "Validation Batch part loss -48.3117\n",
      "Validation Batch part loss -53.7418\n",
      "Validation Batch part loss -52.1415\n",
      "Validation Batch part loss -47.8274\n",
      "Validation Batch part loss -51.0551\n",
      "Validation Batch part loss -49.7983\n",
      "Validation Batch part loss -67.8843\n",
      "Validation Batch part loss -84.7880\n",
      "Validation Batch part loss -58.7221\n",
      "Validation Batch part loss -54.7828\n",
      "Validation Batch part loss -58.5862\n",
      "Validation Batch part loss -55.9998\n",
      "Validation Batch part loss -21.8656\n",
      "Validation Batch part loss -30.1161\n",
      "Validation Batch part loss -100.7004\n",
      "Validation Batch part loss -100.9210\n",
      "Validation Batch part loss -100.2991\n",
      "Validation Batch part loss -103.8420\n",
      "Validation Batch part loss -61.7035\n",
      "Validation Batch part loss -15.8880\n",
      "Validation Batch part loss -43.3909\n",
      "Validation Batch part loss -34.8597\n",
      "Validation Batch part loss -35.7123\n",
      "Validation Batch part loss -37.0352\n",
      "Validation Batch part loss -64.2396\n",
      "Validation Batch part loss -81.8515\n",
      "Validation Batch part loss -75.4078\n",
      "Validation Batch part loss -78.2783\n",
      "Validation Batch part loss -78.3570\n",
      "Validation Batch part loss -113.5151\n",
      "Validation Batch part loss -113.8940\n",
      "Validation Batch part loss -114.8655\n",
      "Validation Batch part loss -86.8004\n",
      "Validation Batch part loss -78.5655\n",
      "Validation Batch part loss -77.2672\n",
      "Validation Batch part loss -81.1318\n",
      "Validation Batch part loss -82.4062\n",
      "Validation Batch part loss -103.6573\n",
      "Validation Batch part loss -82.5881\n",
      "Validation Batch part loss -57.4639\n",
      "Validation Batch part loss -55.4612\n",
      "Validation Batch part loss -78.5169\n",
      "Validation Batch part loss -88.0408\n",
      "Validation Batch part loss -87.0017\n",
      "Validation Batch part loss -86.4979\n",
      "Validation Batch part loss -71.7081\n",
      "Validation Batch part loss -35.8539\n",
      "Validation Batch part loss -55.5563\n",
      "Validation Batch part loss -61.7237\n",
      "Validation Batch part loss -58.5840\n",
      "Validation Batch part loss -64.2816\n",
      "Validation Batch part loss -94.1658\n",
      "Validation Batch part loss -103.8366\n",
      "Validation Batch part loss -42.9323\n",
      "Validation Batch part loss -46.1811\n",
      "Validation Batch part loss -45.4179\n",
      "Validation Batch part loss -42.3015\n",
      "Validation Batch part loss -105.9013\n",
      "Validation Batch part loss -92.2413\n",
      "Validation Batch part loss -70.7858\n",
      "Validation Batch part loss -65.9122\n",
      "Validation Batch part loss -44.1810\n",
      "Validation Batch part loss -42.0108\n",
      "Validation Batch part loss -39.9272\n",
      "Validation Batch part loss -43.1997\n",
      "Validation Batch part loss -27.0668\n",
      "Validation Batch part loss -106.6035\n",
      "Validation Batch part loss -102.5936\n",
      "Validation Batch part loss -104.8342\n",
      "Validation Batch part loss -106.3854\n",
      "Validation Batch part loss -80.4233\n",
      "Validation Batch part loss -51.5593\n",
      "Validation Batch part loss -58.0166\n",
      "Validation Batch part loss -84.5432\n",
      "Validation Batch part loss -82.2237\n",
      "Validation Batch part loss -85.7784\n",
      "Validation Batch part loss -83.9161\n",
      "Validation Batch part loss -81.9506\n",
      "Validation Batch part loss -80.2877\n",
      "Validation Batch part loss -88.3143\n",
      "Validation Batch part loss -88.9417\n",
      "Validation Batch part loss -88.3213\n",
      "Validation Batch part loss -87.7918\n",
      "Validation Batch part loss -89.7964\n",
      "Validation Batch part loss -48.0130\n",
      "Validation Batch part loss -52.9952\n",
      "Validation Batch part loss -51.4203\n",
      "Validation Batch part loss -49.6440\n",
      "Validation Batch part loss -58.5950\n",
      "Validation Batch part loss -70.7545\n",
      "Validation Batch part loss -75.0698\n",
      "Validation Batch part loss -76.2340\n",
      "Validation Batch part loss -71.1463\n",
      "Validation Batch part loss -80.5988\n",
      "Validation Batch part loss -107.9283\n",
      "Validation Batch part loss -72.2849\n",
      "Validation Batch part loss -66.4561\n",
      "Validation Batch part loss -68.7103\n",
      "Validation Batch part loss -71.1297\n",
      "Validation Batch part loss -77.3897\n",
      "Validation Batch part loss -93.3461\n",
      "Validation Batch part loss -94.8838\n",
      "Validation Batch part loss -91.9993\n",
      "Validation Batch part loss -85.7364\n",
      "Validation Batch part loss -67.9227\n",
      "Validation Batch part loss -67.2617\n",
      "Validation Batch part loss -67.2091\n",
      "Validation Batch part loss -48.0725\n",
      "Validation Batch part loss -34.7605\n",
      "Validation Batch part loss -27.9855\n",
      "Validation Batch part loss -44.2829\n",
      "Validation Batch part loss -59.3631\n",
      "Validation Batch part loss -60.5391\n",
      "Validation Batch part loss -62.1440\n",
      "Validation Batch part loss -73.3416\n",
      "Validation Batch part loss -83.5977\n",
      "Validation Batch part loss -98.0046\n",
      "Validation Batch part loss -97.7656\n",
      "Validation Batch part loss -97.0384\n",
      "Validation Batch part loss -82.4271\n",
      "Epoch [1/1], Validation Loss: -75.9302\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "criterion = torch.nn.CosineSimilarity(dim=1)\n",
    "optimizer = optim.Adam(custom_model.parameters(), lr=0.00001)\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    custom_model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    for batch_images, batch_scores in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss = 0\n",
    "        for img, score in zip(batch_images, batch_scores):\n",
    "            vision_features, score_features = custom_model(img.unsqueeze(0), score.unsqueeze(0))  # Adjust dimensions if necessary\n",
    "            cos_sim = criterion(score_features, vision_features)\n",
    "            loss = -cos_sim.mean()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss\n",
    "        print(f\"Batch part loss {batch_loss:.4f}\")\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    custom_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0.0\n",
    "        for batch_images, batch_scores in validation_data_loader:\n",
    "            batch_loss = 0\n",
    "            for img, score in zip(batch_images, batch_scores):\n",
    "                vision_features, score_features = custom_model(img.unsqueeze(0), score.unsqueeze(0))  # Adjust dimensions if necessary\n",
    "                cos_sim = criterion(score_features, vision_features)\n",
    "                loss = -cos_sim.mean()\n",
    "\n",
    "                batch_loss += loss.item()\n",
    "                \n",
    "            validation_loss += batch_loss\n",
    "            print(f\"Validation Batch part loss {batch_loss:.4f}\")\n",
    "        \n",
    "        avg_validation_loss = validation_loss / len(validation_data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_validation_loss:.4f}\")\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(custom_model.state_dict(), \"kaal_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020df69c-d370-4e1d-929f-d6796131f00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f879fb-f99d-444f-b298-c56ea8c19572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd11f8b1-5754-4a25-9ab9-25f83d7d24dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomPLIPModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_projection): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc_layer): Linear(in_features=4, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "# Assuming CustomPLIPModel is defined as before\n",
    "class CustomPLIPModel(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(CustomPLIPModel, self).__init__()\n",
    "        self.vision_model = original_model.vision_model\n",
    "        self.vision_projection = torch.nn.Linear(768, 512)\n",
    "        self.fc_layer = torch.nn.Linear(4, 512)  # Fully connected layer for the 4D vector\n",
    "\n",
    "    def forward(self, pixel_values, score_vector):\n",
    "        vision_output = self.vision_model(pixel_values)\n",
    "        pooled_output = vision_output.pooler_output\n",
    "        vision_features = self.vision_projection(pooled_output)\n",
    "        score_features = self.fc_layer(score_vector)\n",
    "        \n",
    "        return vision_features, score_features\n",
    "    \n",
    "# Reload the model\n",
    "original_model = CLIPVisionModel.from_pretrained(\"../plip/\")\n",
    "custom_model = CustomPLIPModel(original_model)\n",
    "custom_model.load_state_dict(torch.load(\"kaal_model.pth\"))\n",
    "\n",
    "# Ensure the model is in evaluation mode for feature extraction\n",
    "custom_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e02474e-b1f2-45d2-acb9-f40bce79587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class PatientTileDataset(Dataset):\n",
    "    def __init__(self, data_dir, model, save_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.model = model\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.files = []\n",
    "        for patient_id in os.listdir(data_dir):\n",
    "            patient_dir = os.path.join(data_dir, patient_id)\n",
    "            if os.path.isdir(patient_dir):\n",
    "                for f in os.listdir(patient_dir):\n",
    "                    if f.endswith('.pt'):\n",
    "                        self.files.append((os.path.join(patient_dir, f), patient_id))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, patient_id = self.files[idx]\n",
    "        data = torch.load(file_path)\n",
    "        tile_data = torch.from_numpy(data['tile_data'][0]).unsqueeze(0)  # Add batch dimension\n",
    "        # Assuming the model takes a batch of images; if not, you might need to adjust this.\n",
    "        with torch.no_grad():\n",
    "            vision_features, _ = self.model(pixel_values=tile_data, score_vector=torch.zeros(1, 4))\n",
    "        feature_path = self.save_dir / patient_id / os.path.basename(file_path)\n",
    "        feature_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Save vision features\n",
    "        torch.save(vision_features, feature_path)\n",
    "        return feature_path\n",
    "\n",
    "# Assuming you've instantiated your model somewhere as custom_model\n",
    "data_dir = 'plip_preprocess/'\n",
    "save_dir = 'kaal_extract/'\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = PatientTileDataset(data_dir=data_dir, model=custom_model, save_dir=save_dir)\n",
    "\n",
    "# Example of processing and saving features\n",
    "for _ in dataset:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6daf7ad5-913a-4bd0-8541-bac5e6dc6466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a6ed2-e14c-4ab5-bded-666ef6ef4032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3585de3-df29-4f85-8a9b-2a71af51b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4b6e776-fd31-4fda-aa7f-6f18ffc9c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('kaal_extract/TCGA-BA-4074/TCGA-BA-4074_10_1.jpeg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "048ab367-c46b-4fb0-82a2-df9b3771d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df23 =  pd.read_csv('g2_g3.csv')\n",
    "df23_2 = df23.set_index('HNSC')\n",
    "there = set(list(x[:] for x in df23_2.index))\n",
    "wsi_there = os.listdir('kaal_extract/')\n",
    "use = list(there.intersection(wsi_there))\n",
    "df23_2 = df23_2.loc[use]\n",
    "df23_2['cluster'] = df23_2['cluster'] -2\n",
    "\n",
    "df23_3  = df23_2.sample(frac=1)\n",
    "\n",
    "class1 = list(df23_3[df23_3['cluster']==1].index)\n",
    "class0 = list(df23_3[df23_3['cluster']==0].index)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "C1_X_train, C1_X_test = train_test_split(class1, test_size=0.3)\n",
    "C0_X_train, C0_X_test = train_test_split(class0, test_size=0.3)\n",
    "\n",
    "C1_X_validate, C1_X_test = train_test_split(C1_X_test, test_size=0.4)\n",
    "C0_X_validate, C0_X_test = train_test_split(C0_X_test, test_size=0.4)\n",
    "\n",
    "\n",
    "X_train = [];X_train.extend(C1_X_train);X_train.extend(C0_X_train);\n",
    "X_test = [];X_test.extend(C1_X_test);X_test.extend(C0_X_test)\n",
    "X_validate = [];X_validate.extend(C1_X_validate);X_validate.extend(C0_X_validate)\n",
    "\n",
    "random.shuffle(X_train);\n",
    "random.shuffle(X_test)\n",
    "random.shuffle(X_validate);\n",
    "\n",
    "data_info = {};\n",
    "data_info['train'] = X_train;data_info['test'] = X_test;data_info['validate'] = X_validate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e37ae11-aff8-44ac-956c-b357dfcf4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/data_info0315.pkl','wb') as f:\n",
    "    pickle.dump(data_info,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4361d4ea-e4d5-45af-bc0f-434e6cb290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df23_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc284034-13f4-48d1-9196-2b34113d23a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C1 - Train : 70 , Validate : 12 , Test : 18 \n",
      " C0 - Train : 59 , Validate : 11 , Test : 15 \n"
     ]
    }
   ],
   "source": [
    "print(\" C1 - Train : {} , Validate : {} , Test : {} \".format(len(C1_X_train),len(C1_X_test),len(C1_X_validate)))\n",
    "print(\" C0 - Train : {} , Validate : {} , Test : {} \".format(len(C0_X_train),len(C0_X_test),len(C0_X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f21a4-4300-41d9-8ebf-40a51f940648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c1fe1edb-a3d4-4671-abd1-6baba8e119ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {};\n",
    "data['train'] = {};data['test'] = {};data['validate'] = {};\n",
    "data['train']['X'] = [];data['train']['Y'] = []\n",
    "data['test']['X'] = [];data['test']['Y'] = []\n",
    "data['validate']['X'] = [];data['validate']['Y'] = []\n",
    "\n",
    "for i,pID in enumerate(X_train[:]):\n",
    "    fol_p = os.path.join('kaal_extract/',pID) \n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p,tile)\n",
    "        \n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        # print(np1[0].shape)\n",
    "        tile_data.append(np1)\n",
    "        \n",
    "    data['train']['X'].extend(np.array(tile_data))\n",
    "    data['train']['Y'].extend(list(df23_3.loc[pID] for each in range(len(tile_data)) ))\n",
    "    # except:\n",
    "    #     print('not there {}'.format(pID))\n",
    "\n",
    "data['train']['X'] = np.array(data['train']['X']);\n",
    "data['train']['Y'] = np.array(data['train']['Y'])\n",
    "data['train']['X'] = np.squeeze(data['train']['X'], axis=1)\n",
    "\n",
    "\n",
    "for i, pID in enumerate(X_validate[:]):\n",
    "    fol_p = os.path.join('kaal_extract/', pID)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p, tile)\n",
    "\n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        tile_data.append(np1)\n",
    "\n",
    "    data['validate']['X'].extend(np.array(tile_data))\n",
    "    data['validate']['Y'].extend([df23_3.loc[pID] for each in range(len(tile_data))])\n",
    "\n",
    "data['validate']['X'] = np.array(data['validate']['X'])\n",
    "data['validate']['Y'] = np.array(data['validate']['Y'])\n",
    "data['validate']['X'] = np.squeeze(data['validate']['X'], axis=1)\n",
    "\n",
    "\n",
    "for i, pID in enumerate(X_test[:]):\n",
    "    fol_p = os.path.join('kaal_extract/', pID)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p, tile)\n",
    "\n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        tile_data.append(np1)\n",
    "\n",
    "    data['test']['X'].extend(np.array(tile_data))\n",
    "    data['test']['Y'].extend([df23_3.loc[pID] for each in range(len(tile_data))])\n",
    "\n",
    "data['test']['X'] = np.array(data['test']['X'])\n",
    "data['test']['Y'] = np.array(data['test']['Y'])\n",
    "data['test']['X'] = np.squeeze(data['test']['X'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046eb73-b199-435e-beb7-a8b52f513988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "077d7811-28d4-4c20-8f68-7fe48b326cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07806b9f-6579-49e1-9c65-b02914bf5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/data_031524_1.pkl','wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bd00cb15-7bb5-4484-9cd9-8d1f9a70d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_data = {};\n",
    "for pID in df23_3.index:\n",
    "    fol_p = os.path.join('kaal_extract/',pID)\n",
    "    tiles = os.listdir(fol_p) ;\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p,tile);\n",
    "        tile_data.append(torch.load(tile_p).numpy())\n",
    "        \n",
    "    np1 = np.array(tile_data)\n",
    "    wsi_data[pID] = {} ;\n",
    "    wsi_data[pID]['tiles'] = np.squeeze(np1,axis=1)\n",
    "    wsi_data[pID]['class'] = df23_3.loc[pID][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "58b37381-5109-4823-9174-858dde774c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833, 512)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsi_data['TCGA-UF-A71E']['tiles'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "62e16fc4-4c16-421c-8af5-6b6af7a0a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fba54379-63c8-49a7-98df-d972ee19227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/wsi_data_g2_g3.pkl','wb') as f:\n",
    "    pickle.dump(wsi_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb29eb-f301-4453-9ceb-6f5dc85eff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ff48e-30a0-45c4-9ad4-8c9c2110eeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffa131-997d-488c-8c91-3951e0f16378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
