{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471c3f3b-0302-47f2-b685-7748ef442541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71fa24aa-dab3-4902-b6cf-3b944d53b512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLIPModel_Vision(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPVisionModel\n",
    "\n",
    "class PLIPModel_Vision(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PLIPModel_Vision, self).__init__()\n",
    "        self.vision_model = original_model.vision_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_output = self.vision_model(pixel_values)\n",
    "        pooled_output = vision_output.pooler_output\n",
    "        return pooled_output\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"../plip/\")\n",
    "custom_model = PLIPModel_Vision(model)\n",
    "custom_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020df69c-d370-4e1d-929f-d6796131f00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f879fb-f99d-444f-b298-c56ea8c19572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e02474e-b1f2-45d2-acb9-f40bce79587f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class PatientTileDataset(Dataset):\n",
    "    def __init__(self, data_dir, model, save_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.model = model\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.files = []\n",
    "        for patient_id in os.listdir(data_dir):\n",
    "            patient_dir = os.path.join(data_dir, patient_id)\n",
    "            if os.path.isdir(patient_dir):\n",
    "                for f in os.listdir(patient_dir):\n",
    "                    if f.endswith('.pt'):\n",
    "                        self.files.append((os.path.join(patient_dir, f), patient_id))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, patient_id = self.files[idx]\n",
    "        data = torch.load(file_path)\n",
    "        tile_data = torch.from_numpy(data['tile_data'][0]).unsqueeze(0)  # Add batch dimension\n",
    "        # Assuming the model takes a batch of images; if not, you might need to adjust this.\n",
    "        with torch.no_grad():\n",
    "            vision_features = self.model(pixel_values=tile_data)\n",
    "        feature_path = self.save_dir / patient_id / os.path.basename(file_path)\n",
    "        feature_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Save vision features\n",
    "        torch.save(vision_features, feature_path)\n",
    "        return feature_path\n",
    "\n",
    "# Assuming you've instantiated your model somewhere as custom_model\n",
    "data_dir = 'plip_preprocess/'\n",
    "save_dir = 'regular_plip_extracted/'\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = PatientTileDataset(data_dir=data_dir, model=custom_model, save_dir=save_dir)\n",
    "\n",
    "# Example of processing and saving features\n",
    "for _ in dataset:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6daf7ad5-913a-4bd0-8541-bac5e6dc6466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('regular_plip_extracted/TCGA-CQ-5329/TCGA-CQ-5329_10_21.jpeg.pt').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a6ed2-e14c-4ab5-bded-666ef6ef4032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3585de3-df29-4f85-8a9b-2a71af51b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4b6e776-fd31-4fda-aa7f-6f18ffc9c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('kaal_extract/TCGA-BA-4074/TCGA-BA-4074_10_1.jpeg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "048ab367-c46b-4fb0-82a2-df9b3771d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df23 =  pd.read_csv('g2_g3.csv')\n",
    "df23_2 = df23.set_index('HNSC')\n",
    "there = set(list(x[:] for x in df23_2.index))\n",
    "wsi_there = os.listdir('kaal_extract/')\n",
    "use = list(there.intersection(wsi_there))\n",
    "df23_2 = df23_2.loc[use]\n",
    "df23_2['cluster'] = df23_2['cluster'] -2\n",
    "\n",
    "df23_3  = df23_2.sample(frac=1)\n",
    "\n",
    "class1 = list(df23_3[df23_3['cluster']==1].index)\n",
    "class0 = list(df23_3[df23_3['cluster']==0].index)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "C1_X_train, C1_X_test = train_test_split(class1, test_size=0.3)\n",
    "C0_X_train, C0_X_test = train_test_split(class0, test_size=0.3)\n",
    "\n",
    "C1_X_validate, C1_X_test = train_test_split(C1_X_test, test_size=0.4)\n",
    "C0_X_validate, C0_X_test = train_test_split(C0_X_test, test_size=0.4)\n",
    "\n",
    "\n",
    "X_train = [];X_train.extend(C1_X_train);X_train.extend(C0_X_train);\n",
    "X_test = [];X_test.extend(C1_X_test);X_test.extend(C0_X_test)\n",
    "X_validate = [];X_validate.extend(C1_X_validate);X_validate.extend(C0_X_validate)\n",
    "\n",
    "random.shuffle(X_train);\n",
    "random.shuffle(X_test)\n",
    "random.shuffle(X_validate);\n",
    "\n",
    "data_info = {};\n",
    "data_info['train'] = X_train;data_info['test'] = X_test;data_info['validate'] = X_validate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e37ae11-aff8-44ac-956c-b357dfcf4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/data_info0315.pkl','wb') as f:\n",
    "    pickle.dump(data_info,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4361d4ea-e4d5-45af-bc0f-434e6cb290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df23_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc284034-13f4-48d1-9196-2b34113d23a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C1 - Train : 70 , Validate : 12 , Test : 18 \n",
      " C0 - Train : 59 , Validate : 11 , Test : 15 \n"
     ]
    }
   ],
   "source": [
    "print(\" C1 - Train : {} , Validate : {} , Test : {} \".format(len(C1_X_train),len(C1_X_test),len(C1_X_validate)))\n",
    "print(\" C0 - Train : {} , Validate : {} , Test : {} \".format(len(C0_X_train),len(C0_X_test),len(C0_X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f21a4-4300-41d9-8ebf-40a51f940648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c1fe1edb-a3d4-4671-abd1-6baba8e119ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {};\n",
    "data['train'] = {};data['test'] = {};data['validate'] = {};\n",
    "data['train']['X'] = [];data['train']['Y'] = []\n",
    "data['test']['X'] = [];data['test']['Y'] = []\n",
    "data['validate']['X'] = [];data['validate']['Y'] = []\n",
    "\n",
    "for i,pID in enumerate(X_train[:]):\n",
    "    fol_p = os.path.join('kaal_extract/',pID) \n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p,tile)\n",
    "        \n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        # print(np1[0].shape)\n",
    "        tile_data.append(np1)\n",
    "        \n",
    "    data['train']['X'].extend(np.array(tile_data))\n",
    "    data['train']['Y'].extend(list(df23_3.loc[pID] for each in range(len(tile_data)) ))\n",
    "    # except:\n",
    "    #     print('not there {}'.format(pID))\n",
    "\n",
    "data['train']['X'] = np.array(data['train']['X']);\n",
    "data['train']['Y'] = np.array(data['train']['Y'])\n",
    "data['train']['X'] = np.squeeze(data['train']['X'], axis=1)\n",
    "\n",
    "\n",
    "for i, pID in enumerate(X_validate[:]):\n",
    "    fol_p = os.path.join('kaal_extract/', pID)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p, tile)\n",
    "\n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        tile_data.append(np1)\n",
    "\n",
    "    data['validate']['X'].extend(np.array(tile_data))\n",
    "    data['validate']['Y'].extend([df23_3.loc[pID] for each in range(len(tile_data))])\n",
    "\n",
    "data['validate']['X'] = np.array(data['validate']['X'])\n",
    "data['validate']['Y'] = np.array(data['validate']['Y'])\n",
    "data['validate']['X'] = np.squeeze(data['validate']['X'], axis=1)\n",
    "\n",
    "\n",
    "for i, pID in enumerate(X_test[:]):\n",
    "    fol_p = os.path.join('kaal_extract/', pID)\n",
    "    tiles = os.listdir(fol_p)\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p, tile)\n",
    "\n",
    "        np1 = torch.load(tile_p).numpy()\n",
    "        tile_data.append(np1)\n",
    "\n",
    "    data['test']['X'].extend(np.array(tile_data))\n",
    "    data['test']['Y'].extend([df23_3.loc[pID] for each in range(len(tile_data))])\n",
    "\n",
    "data['test']['X'] = np.array(data['test']['X'])\n",
    "data['test']['Y'] = np.array(data['test']['Y'])\n",
    "data['test']['X'] = np.squeeze(data['test']['X'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046eb73-b199-435e-beb7-a8b52f513988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "077d7811-28d4-4c20-8f68-7fe48b326cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07806b9f-6579-49e1-9c65-b02914bf5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/data_031524_1.pkl','wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bd00cb15-7bb5-4484-9cd9-8d1f9a70d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_data = {};\n",
    "for pID in df23_3.index:\n",
    "    fol_p = os.path.join('kaal_extract/',pID)\n",
    "    tiles = os.listdir(fol_p) ;\n",
    "    tile_data = []\n",
    "    for tile in tiles:\n",
    "        tile_p = os.path.join(fol_p,tile);\n",
    "        tile_data.append(torch.load(tile_p).numpy())\n",
    "        \n",
    "    np1 = np.array(tile_data)\n",
    "    wsi_data[pID] = {} ;\n",
    "    wsi_data[pID]['tiles'] = np.squeeze(np1,axis=1)\n",
    "    wsi_data[pID]['class'] = df23_3.loc[pID][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "58b37381-5109-4823-9174-858dde774c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833, 512)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsi_data['TCGA-UF-A71E']['tiles'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "62e16fc4-4c16-421c-8af5-6b6af7a0a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fba54379-63c8-49a7-98df-d972ee19227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/wsi_data_g2_g3.pkl','wb') as f:\n",
    "    pickle.dump(wsi_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb29eb-f301-4453-9ceb-6f5dc85eff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ff48e-30a0-45c4-9ad4-8c9c2110eeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffa131-997d-488c-8c91-3951e0f16378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
