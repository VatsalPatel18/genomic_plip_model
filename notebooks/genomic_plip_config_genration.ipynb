{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c1fccd-4848-4604-8736-8653df4095e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../classes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e58f2cc-f659-410e-a8c9-6d6c773a3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca1e014-c5d1-4943-b529-8b1dadc79f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9223159-3849-4f46-83da-3288b8ef5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genomic_plip_model import GenomicPLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976627a9-2334-4ade-855d-b18a7caa9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd3f351-f01c-4871-9a6a-48f112e20ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model = CLIPVisionModel.from_pretrained(\"../plip/\")\n",
    "genomic_plip = GenomicPLIPModel(original_model)\n",
    "genomic_plip.load_state_dict(torch.load('../models/genomic_plip.pth'))\n",
    "# genomic_plip.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3858ba0-3df7-4cdd-bcb5-aba181b03f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_plip.eval()  # Switch to evaluation mode if needed\n",
    "\n",
    "# Save the model's state_dict\n",
    "model_save_path = '../models/genomic_plip_model.bin'  # Path where you want to save the model\n",
    "torch.save(genomic_plip.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f31c765-932e-48b2-bf5c-b0e92155c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb436c2-ca0f-4a8d-aed8-b3d9e61054ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plip_config =original_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acef195c-11f9-4da0-9e2b-6621cc77f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plip_config_dict = plip_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406dda5b-799b-4934-83a4-b0588d75bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plip_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f20e615-5a1b-4e3f-aff4-d13e89d4536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modifications = {\n",
    "    \"additional_head\": {\n",
    "        \"type\": \"Linear\",\n",
    "        \"in_features\": 768,\n",
    "        \"out_features\": 512\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"type\": \"Linear\",\n",
    "        \"in_features\": 4,\n",
    "        \"out_features\": 512\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08708fe5-d36d-40f1-b910-d95af150876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the base model config with the modifications\n",
    "genomic_plip_config = {\n",
    "    \"model_type\": \"CLIPVisionModel+GenomicPLIP\",\n",
    "    \"base_model_config\": plip_config_dict,\n",
    "    \"modifications\": modifications,\n",
    "    \"weights_source\": \"Hugging Face/your-custom-model\",\n",
    "    \"description\": \"This model is an extension of CLIPVisionModel with a custom head for Genomic PLIP tasks.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19133450-05a1-44b7-9557-ee06bb9b07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../genomic_plip_hug/genomic_plip_config.json', 'w') as fp:\n",
    "    json.dump(genomic_plip_config, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c941b44-b0b0-49bc-a2ac-8e646009bfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_lineage': {'original': 'CLIP',\n",
       "  'intermediate': 'PLIP',\n",
       "  'current': 'Genomic PLIP'},\n",
       " 'current_model_config': {'model_type': 'GenomicPLIPModel',\n",
       "  'description': 'Genomic PLIP, an extension of the PLIP model with additional functionality for genomic analysis tasks.',\n",
       "  'base_model': 'PLIP',\n",
       "  'modifications': {'vision_projection': {'type': 'Linear',\n",
       "    'in_features': 768,\n",
       "    'out_features': 512},\n",
       "   'fc_layer': {'type': 'Linear', 'in_features': 4, 'out_features': 512}}},\n",
       " 'reference_configs': {'CLIP': 'path/to/clip/config.json',\n",
       "  'PLIP': 'path/to/plip/config.json'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"model_lineage\": {\n",
    "        \"original\": \"CLIP\",\n",
    "        \"intermediate\": \"PLIP\",\n",
    "        \"current\": \"Genomic PLIP\"\n",
    "    },\n",
    "    \"current_model_config\": {\n",
    "        \"model_type\": \"GenomicPLIPModel\",\n",
    "        \"description\": \"Genomic PLIP, an extension of the PLIP model with additional functionality for genomic analysis tasks.\",\n",
    "        \"base_model\": \"PLIP\",\n",
    "        \"modifications\": {\n",
    "            \"vision_projection\": {\n",
    "                \"type\": \"Linear\",\n",
    "                \"in_features\": 768,\n",
    "                \"out_features\": 512\n",
    "            },\n",
    "            \"fc_layer\": {\n",
    "                \"type\": \"Linear\",\n",
    "                \"in_features\": 4,\n",
    "                \"out_features\": 512\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"reference_configs\": {\n",
    "        \"CLIP\": \"path/to/clip/config.json\",\n",
    "        \"PLIP\": \"path/to/plip/config.json\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c24d526-4097-4e3d-9ed5-7c4e0ca173da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../genomic_plip_hug/plip_config.json', 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c023e93-d5db-48c6-ac75-ab293515d0ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_commit_hash': None,\n",
       " 'architectures': ['CLIPModel'],\n",
       " 'initializer_factor': 1.0,\n",
       " 'logit_scale_init_value': 2.6592,\n",
       " 'model_type': 'clip',\n",
       " 'projection_dim': 512,\n",
       " 'text_config': {'_name_or_path': '',\n",
       "  'add_cross_attention': False,\n",
       "  'architectures': None,\n",
       "  'attention_dropout': 0.0,\n",
       "  'bad_words_ids': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'bos_token_id': 0,\n",
       "  'chunk_size_feed_forward': 0,\n",
       "  'cross_attention_hidden_size': None,\n",
       "  'decoder_start_token_id': None,\n",
       "  'diversity_penalty': 0.0,\n",
       "  'do_sample': False,\n",
       "  'dropout': 0.0,\n",
       "  'early_stopping': False,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'eos_token_id': 2,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'finetuning_task': None,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'hidden_act': 'quick_gelu',\n",
       "  'hidden_size': 512,\n",
       "  'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'},\n",
       "  'initializer_factor': 1.0,\n",
       "  'initializer_range': 0.02,\n",
       "  'intermediate_size': 2048,\n",
       "  'is_decoder': False,\n",
       "  'is_encoder_decoder': False,\n",
       "  'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "  'layer_norm_eps': 1e-05,\n",
       "  'length_penalty': 1.0,\n",
       "  'max_length': 20,\n",
       "  'max_position_embeddings': 77,\n",
       "  'min_length': 0,\n",
       "  'model_type': 'clip_text_model',\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'num_attention_heads': 8,\n",
       "  'num_beam_groups': 1,\n",
       "  'num_beams': 1,\n",
       "  'num_hidden_layers': 12,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_attentions': False,\n",
       "  'output_hidden_states': False,\n",
       "  'output_scores': False,\n",
       "  'pad_token_id': 1,\n",
       "  'prefix': None,\n",
       "  'problem_type': None,\n",
       "  'projection_dim': 512,\n",
       "  'pruned_heads': {},\n",
       "  'remove_invalid_values': False,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'return_dict': True,\n",
       "  'return_dict_in_generate': False,\n",
       "  'sep_token_id': None,\n",
       "  'suppress_tokens': None,\n",
       "  'task_specific_params': None,\n",
       "  'temperature': 1.0,\n",
       "  'tf_legacy_loss': False,\n",
       "  'tie_encoder_decoder': False,\n",
       "  'tie_word_embeddings': True,\n",
       "  'tokenizer_class': None,\n",
       "  'top_k': 50,\n",
       "  'top_p': 1.0,\n",
       "  'torch_dtype': None,\n",
       "  'torchscript': False,\n",
       "  'transformers_version': '4.26.1',\n",
       "  'typical_p': 1.0,\n",
       "  'use_bfloat16': False,\n",
       "  'vocab_size': 49408},\n",
       " 'torch_dtype': 'float32',\n",
       " 'transformers_version': None,\n",
       " 'vision_config': {'_name_or_path': '',\n",
       "  'add_cross_attention': False,\n",
       "  'architectures': None,\n",
       "  'attention_dropout': 0.0,\n",
       "  'bad_words_ids': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'bos_token_id': None,\n",
       "  'chunk_size_feed_forward': 0,\n",
       "  'cross_attention_hidden_size': None,\n",
       "  'decoder_start_token_id': None,\n",
       "  'diversity_penalty': 0.0,\n",
       "  'do_sample': False,\n",
       "  'dropout': 0.0,\n",
       "  'early_stopping': False,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'eos_token_id': None,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'finetuning_task': None,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'hidden_act': 'quick_gelu',\n",
       "  'hidden_size': 768,\n",
       "  'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'},\n",
       "  'image_size': 224,\n",
       "  'initializer_factor': 1.0,\n",
       "  'initializer_range': 0.02,\n",
       "  'intermediate_size': 3072,\n",
       "  'is_decoder': False,\n",
       "  'is_encoder_decoder': False,\n",
       "  'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "  'layer_norm_eps': 1e-05,\n",
       "  'length_penalty': 1.0,\n",
       "  'max_length': 20,\n",
       "  'min_length': 0,\n",
       "  'model_type': 'clip_vision_model',\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'num_attention_heads': 12,\n",
       "  'num_beam_groups': 1,\n",
       "  'num_beams': 1,\n",
       "  'num_channels': 3,\n",
       "  'num_hidden_layers': 12,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_attentions': False,\n",
       "  'output_hidden_states': False,\n",
       "  'output_scores': False,\n",
       "  'pad_token_id': None,\n",
       "  'patch_size': 32,\n",
       "  'prefix': None,\n",
       "  'problem_type': None,\n",
       "  'projection_dim': 512,\n",
       "  'pruned_heads': {},\n",
       "  'remove_invalid_values': False,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'return_dict': True,\n",
       "  'return_dict_in_generate': False,\n",
       "  'sep_token_id': None,\n",
       "  'suppress_tokens': None,\n",
       "  'task_specific_params': None,\n",
       "  'temperature': 1.0,\n",
       "  'tf_legacy_loss': False,\n",
       "  'tie_encoder_decoder': False,\n",
       "  'tie_word_embeddings': True,\n",
       "  'tokenizer_class': None,\n",
       "  'top_k': 50,\n",
       "  'top_p': 1.0,\n",
       "  'torch_dtype': None,\n",
       "  'torchscript': False,\n",
       "  'transformers_version': '4.26.1',\n",
       "  'typical_p': 1.0,\n",
       "  'use_bfloat16': False}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90bea301-691a-443c-91d1-613609ee6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_type'] = 'genomic_plip'\n",
    "config['architectures'] = ['GenomicPLIPModel']\n",
    "\n",
    "# Since you modified the vision projection, update these settings\n",
    "config['vision_config']['projection_dim'] = 512  # Updated to match your vision_projection layer\n",
    "config['vision_config']['hidden_size'] = 768  # Reflects the input size to your vision_projection\n",
    "\n",
    "# Add custom fields for new layers or significant architecture changes\n",
    "# Assuming you want to detail the new fully connected layer architecture\n",
    "config['genomic_config'] = {\n",
    "    'fc_layer_input': 4,\n",
    "    'fc_layer_output': 512\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09622459-8c2e-4983-99bf-f6cf0f3fd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cba93938-a20f-4c88-9d81-df73c8bbcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../genomic_plip_hug/config.json', 'w') as file:\n",
    "    json.dump(config, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98121c52-c4b2-4e11-8cef-f3c04b0d37d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
